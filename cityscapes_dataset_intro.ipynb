{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import Cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ./cityscapes/leftImg8bit/train/.*\n"
     ]
    }
   ],
   "source": [
    "!rm ./cityscapes/leftImg8bit/train/.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "MEAN = np.array([72.55410438, 81.93415236, 71.4297832]) / 255\n",
    "STD = np.array([51.04788791, 51.76003371, 50.94766331]) / 255\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "class CustomCityScapes(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\", target_type=[\"semantic\"], image_transforms=None, mask_transforms=None):\n",
    "        self.image_transforms = image_transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "        self.target_type = target_type\n",
    "        # ./cityscapes/leftImg8bit/train/leftImg8bit/train <- mac/linux\n",
    "        # .\\\\cityscapes\\\\leftImg8bit\\\\train\\\\leftImg8bit\\\\train <- windows\n",
    "        self.images_split_path = os.path.join(root_dir, \"leftImg8bit\", split)\n",
    "        self.annot_split_path = os.path.join(root_dir, \"gtFine\", split)\n",
    "        \n",
    "        self.image_paths_only = sorted([os.path.join(dir_path, file) \n",
    "                            for city in os.listdir(self.images_split_path) \n",
    "                            for dir_path, _, files in os.walk(os.path.join(self.images_split_path, city)) \n",
    "                            for file in files])\n",
    "        \n",
    "        # \"./cityscapes/leftImg8bit/train/zurich/zurich_000069_000019_leftImg8bit.png\" -> \"zurich_000069_000019\"\n",
    "        self.image_ids = [os.path.split(file)[1].replace(\"_leftImg8bit.png\", \"\") for file in self.image_paths_only]\n",
    "        # \"zurich_000069_000019\": \"./cityscapes/leftImg8bit/train/zurich/zurich_000069_000019_leftImg8bit.png\"\n",
    "        self.image_paths = {id_: file for id_, file in zip(self.image_ids, self.image_paths_only)}\n",
    "\n",
    "        target_type_map = {'semantic': 'labelIds', 'color': 'color', 'polygons': 'polygon', 'instance': 'instanceIds'}\n",
    "        self.target_type = [target_type_map[target] for target in self.target_type]\n",
    "        self.annot_paths = list(zip(*[self.get_annotation_paths(type_=type_) for type_ in self.target_type]))\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for index_ in range(self.__len__()):\n",
    "            yield self.__getitem__(index_)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.image_paths_only[index])\n",
    "        annots = [Image.open(target_item_path) for target_item_path in self.annot_paths[index]]\n",
    "\n",
    "        if self.image_transforms:\n",
    "            img = self.image_transforms(img)\n",
    "        \n",
    "        if self.mask_transforms:\n",
    "            annots = [self.mask_transforms(annot) for annot in annots]\n",
    "\n",
    "        return img, annots\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Custom Cityscapes dataset\"\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "\n",
    "    def get_source_image_paths(self, id_=False):\n",
    "        if not id_:\n",
    "            return self.image_paths\n",
    "        return self.image_paths[id_]\n",
    "    \n",
    "\n",
    "    def get_image(self, id_):\n",
    "        return Image.open(self.image_paths[id_])\n",
    "\n",
    "\n",
    "    def get_annotation_paths(self, type_):\n",
    "        self.annot_path_local = {'polygons': {}, 'color': {}, 'labelIds': {}, 'instanceIds': {}}\n",
    "        for city in os.listdir(self.annot_split_path):\n",
    "            for dir_path, _, files in os.walk(os.path.join(self.annot_split_path, city)):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(dir_path, file)\n",
    "                    file_id, cat = os.path.splitext(os.path.split(file_path)[1].replace('_gtFine_', '|'))[0].split('|')\n",
    "                    self.annot_path_local[cat][file_id] = file_path\n",
    "        \n",
    "        return [self.annot_path_local[type_][id_] for id_ in self.image_ids]\n",
    "\n",
    "\n",
    "# Dataset class ends here\n",
    "\n",
    "def crop_eco_vehicle(image, bottom=-180):\n",
    "    width, height = image.size\n",
    "    return image.crop((0, 0, width, height+bottom))\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda image: crop_eco_vehicle(image)),\n",
    "    transforms.Resize((256, 512), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "    \n",
    "])\n",
    "\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda image: crop_eco_vehicle(image)),\n",
    "    transforms.Resize((256, 512), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "])\n",
    "\n",
    "mask_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda image: crop_eco_vehicle(image)),\n",
    "    transforms.Resize((256, 512), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = CustomCityScapes(\"./cityscapes\", split=\"train\", target_type=[\"color\", \"semantic\"], image_transforms=train_transforms, mask_transforms=mask_transforms)\n",
    "# dataset = CustomCityScapes(\"./cityscapes\", split=\"train\", target_type=[\"color\", \"semantic\"], image_transforms=None, mask_transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the images\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# subplot_idx = 1\n",
    "# for i in range(5):\n",
    "#     im, (color, sem) = dataset[i]\n",
    "#     im = np.array(im).transpose(1, 2, 0)\n",
    "#     # Image\n",
    "#     plt.subplot(5, 3, subplot_idx)\n",
    "#     subplot_idx += 1\n",
    "#     plt.imshow(im)\n",
    "#     plt.axis(\"off\")\n",
    "#     # Color info\n",
    "#     plt.subplot(5, 3, subplot_idx)\n",
    "#     subplot_idx += 1\n",
    "#     plt.imshow(color)\n",
    "#     plt.axis(\"off\")\n",
    "#     # Semantic info\n",
    "#     plt.subplot(5, 3, subplot_idx)\n",
    "#     subplot_idx += 1\n",
    "#     plt.imshow(sem, cmap=\"gray\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
